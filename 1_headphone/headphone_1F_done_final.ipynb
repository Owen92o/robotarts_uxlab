{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa8a8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#헤드폰 연결 및 IMU, UWB 데이터 수치화\n",
    "import time\n",
    "import serial\n",
    "import serial.tools.list_ports \n",
    "\n",
    "for port in serial.tools.list_ports.comports():\n",
    "    print(port.device)\n",
    "\n",
    "ser = serial.Serial(\"/dev/cu.usbserial-110\", 57600)\n",
    "print(ser.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82acd618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_until_full(ser=ser, timeout=None):\n",
    "    ser.timeout = timeout\n",
    "    ser.flush()\n",
    "    ser.flushInput()\n",
    "    while True:\n",
    "        readed = ser.read(size=1)\n",
    "        if readed==b'\\xff':\n",
    "            incomming = ser.read(size=6)\n",
    "            break \n",
    "            \n",
    "    return incomming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e0fa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "hex_incomming = read_until_full()\n",
    "\n",
    "[x for x in hex_incomming]\n",
    "\n",
    "def get_values(hex_incomming):\n",
    "    \"\"\"    x = hex_incomming[0] / 10\n",
    "    y = hex_incomming[1] / 10\"\"\" \n",
    "    x = hex_incomming[1] / 10\n",
    "    y = hex_incomming[0] / 10 \n",
    "    heading = (hex_incomming[2]*255 + hex_incomming[3]) / 100\n",
    "    return x, y, heading\n",
    "\n",
    "get_values(hex_incomming)\n",
    "\n",
    "def get_value_onestep():\n",
    "    hex_incomming = [x for x in read_until_full()]\n",
    "    #print('hex_incomming :', hex_incomming)\n",
    "    return get_values(hex_incomming)\n",
    "\n",
    "get_value_onestep()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b4d8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#라이브러리 선언\n",
    "from flask import Flask, render_template, request, jsonify\n",
    "import os\n",
    "import openai\n",
    "from google.cloud import speech\n",
    "from google.cloud import texttospeech\n",
    "import numpy as np\n",
    "import pyaudio\n",
    "import wave\n",
    "import pygame\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPT-4 사용을 위한 인증키 가져오기\n",
    "openai.api_key = \"Your API Key\"\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"Your JSON Path\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6107eb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flask app 생성\n",
    "app = Flask(__name__)\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c768ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####STT, TTS 음성 대화를 위한 코드\n",
    "\n",
    "#사용자 음성 녹음\n",
    "def record_audio():\n",
    "    CHUNK = 1024\n",
    "    FORMAT = pyaudio.paInt16  # 16 bit\n",
    "    CHANNELS = 1  # mono\n",
    "    RATE = 16000  # sample rate\n",
    "    SILENCE_THRESHOLD = 300  # silence threshold\n",
    "    SILENT_CHUNKS = int(2.5 * RATE / CHUNK)  # 10초 동안 소리 없으면 out\n",
    "    MAX_RECORD_SECONDS = 5  # maximum recording time in seconds\n",
    "    \n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    stream = p.open(format=FORMAT,\n",
    "                    channels=CHANNELS,\n",
    "                    rate=RATE,\n",
    "                    input=True,\n",
    "                    frames_per_buffer=CHUNK)\n",
    "\n",
    "    print(\"* recording\")\n",
    "\n",
    "    frames = []\n",
    "    silent_chunks = 0\n",
    "    audio_started = False\n",
    "    start_time = None\n",
    "\n",
    "    while True:\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data)\n",
    "\n",
    "        # Check for silence\n",
    "        audio_data = np.frombuffer(data, dtype=np.int16)\n",
    "        if np.abs(audio_data).mean() < SILENCE_THRESHOLD:\n",
    "            silent_chunks += 1\n",
    "            if silent_chunks > SILENT_CHUNKS:\n",
    "                break\n",
    "        else:\n",
    "            print(np.abs(audio_data).mean())\n",
    "            silent_chunks = 0   \n",
    "\n",
    "        if audio_started:\n",
    "            if silent_chunks > SILENT_CHUNKS or (time.time() - start_time) >= MAX_RECORD_SECONDS:\n",
    "                break\n",
    "        elif silent_chunks == 0:\n",
    "            audio_started = True\n",
    "            start_time = time.time()\n",
    "\n",
    "    print(\"* done recording\")\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "\n",
    "    # Save the recorded audio to a WAV file\n",
    "    wf = wave.open(\"audio.wav\", \"wb\")\n",
    "    wf.setnchannels(CHANNELS)\n",
    "    wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "    wf.setframerate(RATE)\n",
    "    wf.writeframes(b\"\".join(frames))\n",
    "    wf.close()\n",
    "\n",
    "    # Return the recorded audio data\n",
    "    return b\"\".join(frames)\n",
    "\n",
    "## Byte 오디오 데이터를 한글 text로 변환 STT\n",
    "def transcribe_audio(audio_data):\n",
    "    audio = speech.RecognitionAudio(content=audio_data)\n",
    "    config = speech.RecognitionConfig(\n",
    "        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "        sample_rate_hertz=16000,\n",
    "        language_code=\"ko-KR\"\n",
    "    )\n",
    "\n",
    "    response = stt_client.recognize(config=config, audio=audio)\n",
    "    \n",
    "    if response.results:\n",
    "        return response.results[0].alternatives[0].transcript\n",
    "    else:\n",
    "        print(\"음성 인식에 실패했습니다. 다시 시도해주세요.\")\n",
    "        return None\n",
    "    \n",
    "## 텍스트를 음성으로 변환\n",
    "def synthesize_speech(text, voice_index=0):\n",
    "    # Ensure the provided index is valid\n",
    "    if voice_index < 0 or voice_index >= len(speech_params_list):\n",
    "        raise ValueError(\"Invalid voice index provided\")\n",
    "\n",
    "    # Choose the voice settings based on the provided index\n",
    "    speech_params = speech_params_list[voice_index]\n",
    "\n",
    "    # Convert the ssml_gender string to the corresponding enum value\n",
    "    ssml_gender_map = {\n",
    "        \"MALE\": texttospeech.SsmlVoiceGender.MALE,\n",
    "        \"FEMALE\": texttospeech.SsmlVoiceGender.FEMALE,\n",
    "    }\n",
    "    ssml_gender = ssml_gender_map[speech_params[\"ssml_gender\"]]\n",
    "\n",
    "    input_text = texttospeech.SynthesisInput(text=text)\n",
    "    voice = texttospeech.VoiceSelectionParams(\n",
    "        language_code=\"ko-KR\",\n",
    "        ssml_gender=ssml_gender,  # Use the enum value instead of string\n",
    "        name=speech_params[\"voice_name\"]\n",
    "    )\n",
    "    \n",
    "    audio_config = texttospeech.AudioConfig(\n",
    "        audio_encoding=texttospeech.AudioEncoding.LINEAR16,\n",
    "        speaking_rate=speech_params[\"speaking_rate\"],\n",
    "        pitch=speech_params[\"pitch\"]\n",
    "    )\n",
    "\n",
    "    response = tts_client.synthesize_speech(\n",
    "        input=input_text, voice=voice, audio_config=audio_config\n",
    "    )\n",
    "\n",
    "    return response.audio_content\n",
    "\n",
    "## pyaudio를 사용하여 음성을 재생하는 함수\n",
    "def play_audio(audio_data):\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(\n",
    "        format=p.get_format_from_width(2),\n",
    "        channels=1,\n",
    "        rate=24000,\n",
    "        output=True,\n",
    "    )\n",
    "    stream.write(audio_data)\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "\n",
    "# Set up STT and TTS API clients 변환된 데이터값 변수로 받기\n",
    "stt_client = speech.SpeechClient()\n",
    "tts_client = texttospeech.TextToSpeechClient()\n",
    "\n",
    "# 프롬프트설계, 과거기억 데이터 확인\n",
    "messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e767f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 목소리 설정하기\n",
    "# 남자 목소리 name 변수 수정 = ko-KR-Neural2-C, ko-KR-Standard-C(D), ko-KR-Wavenet-C(D)\n",
    "# 여자 목소리 name 변수 수정 = ko-KR-Neural2-A(B), ko-KR-Standard-A(B), ko-KR-Wavenet-A(B)\n",
    "# language_code=\"ko-KR\",ssml_gender=texttospeech.SsmlVoiceGender.FEMALE, name=\"ko-KR-Standard-A\"\n",
    "# language_code=\"ko-KR\",ssml_gender=texttospeech.SsmlVoiceGender.MALE, name=\"ko-KR-Neural2-C\"\n",
    "\n",
    "speech_params_list = [\n",
    "    {   #0 \n",
    "        \"speaking_rate\": 1.1,\n",
    "        \"pitch\": -2.0,\n",
    "        \"ssml_gender\": \"MALE\",\n",
    "        \"voice_name\": \"ko-KR-Standard-C\"\n",
    "    },\n",
    "    {   #1 \n",
    "        \"speaking_rate\": 1.0,\n",
    "        \"pitch\": 0.0,\n",
    "        \"ssml_gender\": \"FEMALE\",\n",
    "        \"voice_name\": \"ko-KR-Standard-A\"\n",
    "    } \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb42b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#while문 사용을 위한 헤딩값 재계산\n",
    "def get_heading():\n",
    "    return get_value_onestep()[2]\n",
    "\n",
    "#목표 스팟에 대한 거리값 계산 - 방향과 거리에 대한 큐를 주기 위한 값\n",
    "def get_distance1(x, y):\n",
    "    x1, y1 = 8.0, 3.3  # 어른\n",
    "    return math.sqrt((x1 - x) ** 2 + (y1 - y) ** 2)\n",
    "def get_distance2(x, y):\n",
    "    x2, y2 = 5.8, 0.7  # 마음\n",
    "    return math.sqrt((x2 - x) ** 2 + (y2 - y) ** 2)\n",
    "def get_distance3(x, y):\n",
    "    x3, y3 = 4.2, 4.2  # 재구성\n",
    "    return math.sqrt((x3 - x) ** 2 + (y3 - y) ** 2)\n",
    "def get_distance4(x, y):\n",
    "    x4, y4 = 2.0, 2.2  # 해체\n",
    "    return math.sqrt((x4 - x) ** 2 + (y4 - y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe2ec05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zone 정보 및 좌표 설정\n",
    "zones_f1 = {\n",
    "    #구역\n",
    "    \"Zone1\": [(9.4, 0.0), (11.0, 6.5)],  # 시간\n",
    "    \"Zone1-1\": [(7.2, 4.3), (9.4, 6.5)],   # 시간\n",
    "    \"Zone2\": [(7.2, 0.0), (9.4, 4.3)],   # 어른\n",
    "    \"Zone3\": [(3.5, 0.0), (7.0, 4.0)],   # 마음\n",
    "    \"Zone4\": [(0.0, 3.5), (4.6, 6.5)],   # 재구성 4.6, 4.6\n",
    "    \"Zone5\": [(0.0, 0.0), (3.5, 3.5)]   # 해체\n",
    "} \n",
    "# 사운드 파일 재생 여부를 추적하기 위한 딕셔너리\n",
    "sound_played = {\n",
    "    \"rec2_1\": False,\n",
    "    \"rec2_2\": False,\n",
    "    \"rec2_3\": False,\n",
    "    \"rec3_1\": False,\n",
    "    \"rec3_2\": False,\n",
    "    \"rec3_3\": False,\n",
    "    \"rec4_1\": False,\n",
    "    \"rec4_2\": False,\n",
    "    \"rec5_1\": False,\n",
    "    \"rec5_2\": False\n",
    "}\n",
    "conv_executed = {\n",
    "    \"conv1\": False,\n",
    "    \"conv2\": False,\n",
    "    \"conv3\": False,\n",
    "    \"conv4\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_bgm(path, volume=0.5):\n",
    "    pygame.mixer.init()\n",
    "    pygame.mixer.music.load(path)\n",
    "    pygame.mixer.music.set_volume(volume)  # 볼륨 설정\n",
    "    pygame.mixer.music.play(0)  # -1은 무한 반복, 0은 한번 재생을 의미\n",
    "\n",
    "def play_conv(path, callback=None):\n",
    "    pygame.mixer.init()\n",
    "    pygame.mixer.music.load(path)\n",
    "    pygame.mixer.music.play(0)  # 0은 파일을 한 번만 재생합니다.\n",
    "\n",
    "    while pygame.mixer.music.get_busy():\n",
    "        time.sleep(0.1)  # 음악 재생 상태 확인을 위한 잠시 대기\n",
    "\n",
    "    if callback:  # 재생이 완료된 후 실행할 함수가 있다면 호출\n",
    "        callback()\n",
    "\n",
    "def stop_bgm():\n",
    "    pygame.mixer.music.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16e34ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zone(x, y):\n",
    "    for zone, (start, end) in zones_f1.items():\n",
    "        if start[0] <= x <= end[0] and start[1] <= y <= end[1]:  # 조건 수정\n",
    "            return zone\n",
    "    return None\n",
    "\n",
    "zone_N = get_zone(get_value_onestep()[0], get_value_onestep()[1])\n",
    "print(get_value_onestep()[0], get_value_onestep()[1])\n",
    "print(zone_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702fbf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간 \n",
    "rec1_bgm = \"File Path\"\n",
    "# 어른 녹음본 2개 각도\n",
    "rec2_1 = \"File Path\"\n",
    "rec2_2 = \"File Path\"\n",
    "rec2_3 = \"File Path\"\n",
    "rec2_bgm = \"File Path\"\n",
    "# 마음 녹음본 2개 각도\n",
    "rec3_1 = \"File Path\"\n",
    "rec3_2 = \"File Path\"\n",
    "rec3_3 = \"File Path\"\n",
    "rec3_bgm = \"File Path\"\n",
    "# 재구성 녹음본 2개 각도\n",
    "rec4_1 = \"File Path\"\n",
    "rec4_2 = \"File Path\"\n",
    "rec4_bgm = \"File Path\"\n",
    "# 해제 녹음본 2개 각도\n",
    "rec5_1 = \"File Path\"\n",
    "rec5_2 = \"File Path\"\n",
    "rec5_bgm = \"File Path\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d9677d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import io\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39353cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#elevenlabs\n",
    "def synthesize(text):\n",
    "    url = \"https://api.elevenlabs.io/v1/text-to-speech/\"\n",
    "    headers = {\n",
    "        \"Accept\": \"audio/mpeg\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"xi-api-key\": \"You API Key\"  # 여기에 실제 API 키를 입력해야 합니다. 내 키\n",
    "    }\n",
    "    data = {\n",
    "        \"text\": text,\n",
    "        \"model_id\": \"eleven_multilingual_v2\",\n",
    "        \"voice_settings\": {\n",
    "            \"stability\": 0.5,\n",
    "            \"similarity_boost\": 0.75,\n",
    "            \"style_exaggeration\": 0.0\n",
    "        }\n",
    "    }\n",
    "    response = requests.post(url, json=data, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        audio_stream = io.BytesIO(response.content)\n",
    "        audio = AudioSegment.from_file(audio_stream, format=\"mp3\")\n",
    "        play(audio)\n",
    "    else:\n",
    "        print(\"Error: \", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d2c46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPT 사용 정의\n",
    "assistant_role = \"닥터로만 실험실 가이드, 아이\"\n",
    "assistant_persona = \"무섭고 궁금증을 자아내는\"\n",
    "user_role = \"방문객\"\n",
    "messages.append({\"role\": \"system\", \"content\": f\"GPT(:{assistant_role})와 사용자(:{user_role})의 역할극을 시작합니다. {assistant_role}은 {assistant_persona} 성격으로 '닥터로만 실험실' 관람에 대해 스몰토크를 합니다.\"})\n",
    "assistant_content = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#음향 재생 후 대화\n",
    "def conv1():  \n",
    "    if conv_executed[\"conv1\"]:\n",
    "        return  # 이미 실행된 경우 함수를 종료\n",
    "\n",
    "    print(\"어른 대화 1 \")\n",
    "    conv_1 = \"안녕하세요. 아이 입니다. 박사님은 집요하게, 인간의 의식에 대해 집착을 했어요. 당신은 당신의 의식을 완전히 통제할 수 있다고 믿나요?\"\n",
    "    synthesize(conv_1) #elevenlabs API TTS\n",
    "    messages = [{\"role\": \"system\", \"content\": conv_1}] #conv_GPT 추가\n",
    "\n",
    "    print(\"어른 대화 1-1 \")\n",
    "    user_audio = record_audio()\n",
    "    user_content = transcribe_audio(user_audio)\n",
    "    print(user_content)\n",
    "\n",
    "    if not user_content:\n",
    "        user_content = \"모르겠어.\"\n",
    "    messages.append({\"role\": \"system\", \"content\": \"다음 아래 사용자 발화에 대해 총 두 문장으로 대답해줘. 마지막 문장은 질문으로 대답해줘\"})\n",
    "    messages.append({\"role\": \"user\", \"content\": user_content}) #사용자 발화 추가\n",
    "\n",
    "    # 대답1 Use GPT API to generate response\n",
    "    completion = openai.ChatCompletion.create(model=\"gpt-4\", messages=messages)\n",
    "    assistant_content = completion.choices[0].message[\"content\"].strip()\n",
    "    messages.append({\"role\": \"assistant\", \"content\": f\"{assistant_content}\"})\n",
    "    print(f\"S.I : {assistant_content}\")\n",
    "\n",
    "    print(\"어른 대화 2 \")\n",
    "    synthesize(assistant_content) #elevenlabs API TTS\n",
    "\n",
    "    print(\"어른 대화 2-1 \")\n",
    "    user_audio = record_audio()\n",
    "    user_content = transcribe_audio(user_audio)\n",
    "    print(user_content)\n",
    "\n",
    "    print(\"어른 대화 3 \")\n",
    "    ## 대답2 Use GPT API to generate response\n",
    "    completion = openai.ChatCompletion.create(model=\"gpt-4\", messages=messages)\n",
    "    assistant_content = completion.choices[0].message[\"content\"].strip()\n",
    "    messages.append({\"role\": \"assistant\", \"content\": f\"{assistant_content}\"})\n",
    "    print(f\"S.I : {assistant_content}\")\n",
    "    synthesize(assistant_content) #elevenlabs API TTS\n",
    "\n",
    "    user_audio = record_audio()\n",
    "    user_content = transcribe_audio(user_audio)\n",
    "    return None \n",
    "\n",
    "def conv2():  \n",
    "    if conv_executed[\"conv2\"]:\n",
    "        return  # 이미 실행된 경우 함수를 종료\n",
    "\n",
    "    print(\"마음 대화 1 \")\n",
    "    conv_1 = \"이 조각상은 마음이라는 작품이에요. 음.. 박사님은 인간의 마음조차 재현할 수 있다고 믿으셨죠. 당신은 마음이란 것이 존재한다고 믿으시나요?\"\n",
    "    synthesize(conv_1) #elevenlabs API TTS\n",
    "    messages = [{\"role\": \"system\", \"content\": conv_1}] #conv_GPT 추가\n",
    "\n",
    "    print(\"마음 대화 1-1 \")\n",
    "    user_audio = record_audio()\n",
    "    user_content = transcribe_audio(user_audio)\n",
    "    print(user_content)\n",
    "\n",
    "    if not user_content:\n",
    "        user_content = \"모르겠어.\"\n",
    "    messages.append({\"role\": \"system\", \"content\": \"다음 아래 사용자 발화에 대해 총 두 문장으로 대답해줘. 마지막 문장은 질문으로 대답해줘\"})\n",
    "    messages.append({\"role\": \"user\", \"content\": user_content}) #사용자 발화 추가\n",
    "\n",
    "    # 대답1 Use GPT API to generate response\n",
    "    completion = openai.ChatCompletion.create(model=\"gpt-4\", messages=messages)\n",
    "    assistant_content = completion.choices[0].message[\"content\"].strip()\n",
    "    messages.append({\"role\": \"assistant\", \"content\": f\"{assistant_content}\"})\n",
    "    print(f\"S.I : {assistant_content}\")\n",
    "\n",
    "    print(\"마음 대화 2 \")\n",
    "    synthesize(assistant_content) #elevenlabs API TTS\n",
    "\n",
    "    print(\"마음 대화 2-1 \")\n",
    "    user_audio = record_audio()\n",
    "    user_content = transcribe_audio(user_audio)\n",
    "    print(user_content)\n",
    "\n",
    "    print(\"마음 대화 3 \")\n",
    "    ## 대답2 Use GPT API to generate response\n",
    "    completion = openai.ChatCompletion.create(model=\"gpt-4\", messages=messages)\n",
    "    assistant_content = completion.choices[0].message[\"content\"].strip()\n",
    "    messages.append({\"role\": \"assistant\", \"content\": f\"{assistant_content}\"})\n",
    "    print(f\"S.I : {assistant_content}\")\n",
    "    synthesize(assistant_content) #elevenlabs API TTS\n",
    "\n",
    "    user_audio = record_audio()\n",
    "    user_content = transcribe_audio(user_audio)\n",
    "    return None \n",
    "\n",
    "def conv3():  \n",
    "    if conv_executed[\"conv3\"]:\n",
    "        return  # 이미 실행된 경우 함수를 종료\n",
    "\n",
    "    print(\"재구성 대화 1 \")\n",
    "    conv_1 = \"이 작품의 이름은 재구성 입니다. 뉴런 신호를 데이터화 한다는 박사님의 영혼이 담겨있죠. 당신은 이 세상이 실재한다고 믿나요?, 결국 이 세상도 하나의 가상 현실이 아닐까요?\"\n",
    "    synthesize(conv_1) #elevenlabs API TTS\n",
    "    messages = [{\"role\": \"system\", \"content\": conv_1}] #conv_GPT 추가\n",
    "\n",
    "    print(\"재구성 대화 1-1 \")\n",
    "    user_audio = record_audio()\n",
    "    user_content = transcribe_audio(user_audio)\n",
    "    print(user_content)\n",
    "\n",
    "    if not user_content:\n",
    "        user_content = \"모르겠어.\"\n",
    "    messages.append({\"role\": \"system\", \"content\": \"다음 아래 사용자 발화에 대해 총 두 문장으로 대답해줘. 마지막 문장은 질문으로 대답해줘\"})\n",
    "    messages.append({\"role\": \"user\", \"content\": user_content}) #사용자 발화 추가\n",
    "\n",
    "    # 대답1 Use GPT API to generate response\n",
    "    completion = openai.ChatCompletion.create(model=\"gpt-4\", messages=messages)\n",
    "    assistant_content = completion.choices[0].message[\"content\"].strip()\n",
    "    messages.append({\"role\": \"assistant\", \"content\": f\"{assistant_content}\"})\n",
    "    print(f\"S.I : {assistant_content}\")\n",
    "\n",
    "    print(\"재구성 대화 2 \")\n",
    "    synthesize(assistant_content) #elevenlabs API TTS\n",
    "\n",
    "    print(\"재구성 대화 2-1 \")\n",
    "    user_audio = record_audio()\n",
    "    user_content = transcribe_audio(user_audio)\n",
    "    print(user_content)\n",
    "\n",
    "    print(\"재구성 대화 3 \")\n",
    "    ## 대답2 Use GPT API to generate response\n",
    "    completion = openai.ChatCompletion.create(model=\"gpt-4\", messages=messages)\n",
    "    assistant_content = completion.choices[0].message[\"content\"].strip()\n",
    "    messages.append({\"role\": \"assistant\", \"content\": f\"{assistant_content}\"})\n",
    "    print(f\"S.I : {assistant_content}\")\n",
    "    synthesize(assistant_content) #elevenlabs API TTS\n",
    "\n",
    "    user_audio = record_audio()\n",
    "    user_content = transcribe_audio(user_audio)\n",
    "    return None \n",
    "\n",
    "def conv4():  \n",
    "    if conv_executed[\"conv4\"]:\n",
    "        return  # 이미 실행된 경우 함수를 종료\n",
    "\n",
    "    print(\"해체 대화 1 \")\n",
    "    conv_1 = \"몸과 팔, 다리. 이 작품은 기계와 인간을 같은 레이어로 바라보는 박사님의 사상이 담겨있습니다. 당신도 스스로를 기계같다고 느낀 적이 있나요?\"\n",
    "    synthesize(conv_1) #elevenlabs API TTS\n",
    "    messages = [{\"role\": \"system\", \"content\": conv_1}] #conv_GPT 추가\n",
    "\n",
    "    print(\"해체 대화 1-1 \")\n",
    "    user_audio = record_audio()\n",
    "    user_content = transcribe_audio(user_audio)\n",
    "    print(user_content)\n",
    "\n",
    "    if not user_content:\n",
    "        user_content = \"모르겠어.\"\n",
    "    messages.append({\"role\": \"system\", \"content\": \"다음 아래 사용자 발화에 대해 총 두 문장으로 대답해줘. 마지막 문장은 질문으로 대답해줘\"})\n",
    "    messages.append({\"role\": \"user\", \"content\": user_content}) #사용자 발화 추가\n",
    "\n",
    "    # 대답1 Use GPT API to generate response\n",
    "    completion = openai.ChatCompletion.create(model=\"gpt-4\", messages=messages)\n",
    "    assistant_content = completion.choices[0].message[\"content\"].strip()\n",
    "    messages.append({\"role\": \"assistant\", \"content\": f\"{assistant_content}\"})\n",
    "    print(f\"S.I : {assistant_content}\")\n",
    "\n",
    "    print(\"해체 대화 2 \")\n",
    "    synthesize(assistant_content) #elevenlabs API TTS\n",
    "\n",
    "    print(\"해체 대화 2-1 \")\n",
    "    user_audio = record_audio()\n",
    "    user_content = transcribe_audio(user_audio)\n",
    "    print(user_content)\n",
    "\n",
    "    print(\"해체 대화 3 \")\n",
    "    ## 대답2 Use GPT API to generate response\n",
    "    completion = openai.ChatCompletion.create(model=\"gpt-4\", messages=messages)\n",
    "    assistant_content = completion.choices[0].message[\"content\"].strip()\n",
    "    messages.append({\"role\": \"assistant\", \"content\": f\"{assistant_content}\"})\n",
    "    print(f\"S.I : {assistant_content}\")\n",
    "    synthesize(assistant_content) #elevenlabs API TTS\n",
    "\n",
    "    user_audio = record_audio()\n",
    "    user_content = transcribe_audio(user_audio)\n",
    "    return None \n",
    "\n",
    "conv_end1_counter = 0\n",
    "\n",
    "def conv_end():\n",
    "    global conv_end1_counter\n",
    "    conv_end1_counter += 1\n",
    "\n",
    "    if conv_end1_counter == 1:\n",
    "        print(\"작품1 관람 완료\")\n",
    "        conv_continue = \"이 작품은 여기까지하고 다른 작품도 둘러보기로 하죠. 또 다른 박사님의 생각을 읽을 수 있을 겁니다.\"\n",
    "        synthesize(conv_continue)  # elevenlabs API TTS\n",
    "    elif conv_end1_counter == 2:\n",
    "        print(\"작품2 관람 완료\")\n",
    "        conv_continue = \"음.. 저는 잘 모르겠습니다. 다른 작품도 천천히 살펴 보시죠.\"\n",
    "        synthesize(conv_continue)  # elevenlabs API TTS\n",
    "    elif conv_end1_counter == 3:\n",
    "        print(\"작품3 관람 완료\")\n",
    "        conv_continue = \"무슨 생각을 하시든, 저에게는 새롭네요. 남은 작품도 같이 볼까요.\"\n",
    "        synthesize(conv_continue)  # elevenlabs API TTS\n",
    "    elif conv_end1_counter == 4:\n",
    "        conv_end = \"준비된 작품들은 여기까지입니다. 저희 새로운 조수들에게 헤드셋을 반납해 주세요.\"\n",
    "        synthesize(conv_end)  # elevenlabs API TTS\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2461bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_g = \"안녕하세요. 반갑습니다. 경험 시작 전, 내레이션이 들리면 손가락으로 동그라미를 표시해 주세요!\"\n",
    "start_guide = synthesize_speech(conv_g, voice_index=1)\n",
    "play_audio(start_guide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0ea24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_bgm = None  # 현재 재생 중인 배경 음악을 추적하기 위한 변수\n",
    "current_zone = None  # 이전 zone을 추적하기 위한 변수\n",
    "previous_zone = None\n",
    "\n",
    "conv_s = \"안녕하세요. 인공지능 시스템 아이입니다. 이곳에는 박사님이 만든 여러 조각 작품이 있어요. 작품 근처에서는 박사님의 생각을 들을 수 있습니다...,박사님의 생각을 듣다가 중간중간 저와 대화를 나누어 보아요. 작품 주변을 천천히 한바퀴 둘러보시며 소리에 귀기울여 보시기 바래요. 즐거운 관람 되시기 바랍니다.\"\n",
    "synthesize(conv_s)\n",
    "\n",
    "while True:\n",
    "    array_t = get_value_onestep()\n",
    "    pre_x = array_t[0]\n",
    "    pre_y = array_t[1]\n",
    "    heading = array_t[2]\n",
    "\n",
    "    current_zone = get_zone(pre_x, pre_y)\n",
    "    \"\"\"dis1 = get_distance1(pre_x, pre_y)\n",
    "    dis2 = get_distance2(pre_x, pre_y)\n",
    "    dis3 = get_distance3(pre_x, pre_y)\n",
    "    dis4 = get_distance4(pre_x, pre_y)\"\"\"\n",
    "\n",
    "    print(\"헤딩:\", heading) #연구실 환경에 맞게 0도를 화이트보드 방향으로 \n",
    "    print(\"좌표:\", pre_x, \",\", pre_y)\n",
    "    #print(\"스팟과의 거리:\",dis1, \",\", dis2, \",\", dis3, \",\", dis4)\n",
    "    print(f\"현재위치: {current_zone}.\")\n",
    "\n",
    "    # 현재 Zone 결정 및 이전 Zone 추적\n",
    "    if current_zone != previous_zone:\n",
    "        previous_zone = current_zone  # 현재 Zone을 이전 Zone으로 업데이트\n",
    "\n",
    "        if current_zone == \"Zone1\":\n",
    "            play_bgm(rec1_bgm)\n",
    "            \n",
    "        elif current_zone == \"Zone2\":  #어른\n",
    "            print(\"check1\")\n",
    "            play_bgm(rec2_bgm)\n",
    "            print(\"check2\")\n",
    "            time.sleep(5.0)\n",
    "            if not sound_played[\"rec2_1\"]:\n",
    "                sound_played[\"rec2_1\"] = True\n",
    "                play_conv(rec2_1)\n",
    "            if 0 <= heading <= 180 and not sound_played[\"rec2_2\"]:\n",
    "                sound_played[\"rec2_2\"] = True\n",
    "                stop_bgm()\n",
    "                play_conv(rec2_2)\n",
    "                time.sleep(2.0)\n",
    "                play_conv(rec2_3)\n",
    "                play_bgm(rec2_bgm)\n",
    "                conv1()\n",
    "                conv_end()\n",
    "            else:\n",
    "                play_bgm(rec2_bgm)\n",
    "\n",
    "        elif current_zone == \"Zone3\":  #마음\n",
    "            play_bgm(rec3_bgm)\n",
    "            print(\"check3\")\n",
    "            time.sleep(5.0)\n",
    "            if not sound_played[\"rec3_1\"]:\n",
    "                sound_played[\"rec3_1\"] = True\n",
    "                play_conv(rec3_1)\n",
    "            if 0 <= heading <= 180 and not sound_played[\"rec3_2\"]:\n",
    "                sound_played[\"rec3_2\"] = True\n",
    "                stop_bgm()\n",
    "                play_conv(rec3_2)\n",
    "                time.sleep(2.0)\n",
    "                play_conv(rec3_3)\n",
    "                play_bgm(rec3_bgm)\n",
    "                conv2()\n",
    "                conv_end()\n",
    "            else:\n",
    "                play_bgm(rec3_bgm)\n",
    "                \n",
    "        elif current_zone == \"Zone4\":  #재구성\n",
    "            play_bgm(rec4_bgm)\n",
    "            print(\"check4\")\n",
    "            time.sleep(5.0)\n",
    "            if not sound_played[\"rec4_1\"]:\n",
    "                sound_played[\"rec4_1\"] = True\n",
    "                play_conv(rec4_1)\n",
    "            if 0 <= heading <= 180 and not sound_played[\"rec4_2\"]:\n",
    "                sound_played[\"rec4_2\"] = True\n",
    "                stop_bgm()\n",
    "                play_conv(rec4_2)\n",
    "                play_bgm(rec4_bgm)\n",
    "                conv3()\n",
    "                conv_end()\n",
    "            else:\n",
    "                play_bgm(rec4_bgm)\n",
    "                    \n",
    "        elif current_zone == \"Zone5\":  #해체\n",
    "            play_bgm(rec5_bgm)\n",
    "            print(\"check5\")\n",
    "            time.sleep(5.0)\n",
    "            play_conv(rec5_1)\n",
    "            if not sound_played[\"rec5_1\"]:\n",
    "                sound_played[\"rec5_1\"] = True\n",
    "                play_conv(rec5_1)\n",
    "            if 0 <= heading <= 180 and not sound_played[\"rec5_2\"]:\n",
    "                sound_played[\"rec5_2\"] = True\n",
    "                stop_bgm()\n",
    "                play_conv(rec5_2)\n",
    "                play_bgm(rec5_bgm)\n",
    "                conv4()\n",
    "                conv_end()\n",
    "            else:\n",
    "                play_bgm(rec5_bgm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_bgm()\n",
    "#stop_sound_effect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700bf10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_bgm = None  # 현재 재생 중인 배경 음악을 추적하기 위한 변수\n",
    "current_zone = None  # 이전 zone을 추적하기 위한 변수\n",
    "previous_zone = None\n",
    "while True:\n",
    "    array_t = get_value_onestep()\n",
    "    pre_x = array_t[0]\n",
    "    pre_y = array_t[1]\n",
    "    heading = array_t[2]\n",
    "\n",
    "    current_zone = get_zone(pre_x, pre_y)\n",
    "    \"\"\"dis1 = get_distance1(pre_x, pre_y)\n",
    "    dis2 = get_distance2(pre_x, pre_y)\n",
    "    dis3 = get_distance3(pre_x, pre_y)\n",
    "    dis4 = get_distance4(pre_x, pre_y)\"\"\"\n",
    "\n",
    "    print(\"헤딩:\", heading) #연구실 환경에 맞게 0도를 화이트보드 방향으로 \n",
    "    print(\"좌표:\", pre_x, \",\", pre_y)\n",
    "    #print(\"스팟과의 거리:\",dis1, \",\", dis2, \",\", dis3, \",\", dis4)\n",
    "    print(f\"현재위치: {current_zone}.\")\n",
    "\n",
    "    # 현재 Zone 결정 및 이전 Zone 추적\n",
    "    if current_zone != previous_zone:\n",
    "        previous_zone = current_zone  # 현재 Zone을 이전 Zone으로 업데이트\n",
    "\n",
    "        if current_zone == \"Zone1\":\n",
    "            play_bgm(rec1_bgm)\n",
    "            \n",
    "        elif current_zone == \"Zone2\":  #어른\n",
    "            print(\"check2\")\n",
    "            play_bgm(rec2_bgm)\n",
    "        elif current_zone == \"Zone3\":  #어른\n",
    "            print(\"check3\")\n",
    "            play_bgm(rec3_bgm)\n",
    "        elif current_zone == \"Zone4\":  #어른\n",
    "            print(\"check4\")\n",
    "            play_bgm(rec4_bgm)\n",
    "        elif current_zone == \"Zone5\":  #어른\n",
    "            print(\"check5\")\n",
    "            play_bgm(rec5_bgm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88936632",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
