{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, render_template, request, jsonify\n",
    "import os\n",
    "import openai\n",
    "from google.cloud import speech \n",
    "from google.cloud import texttospeech\n",
    "import numpy as np\n",
    "import pyaudio\n",
    "import time\n",
    "import wave\n",
    "import pygame\n",
    "import requests\n",
    "import io\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    #GPT-4 사용을 위한 인증키 가져오기
    openai.api_key = "Your API Key"
    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "Your Json File Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flask app 생성\n",
    "app = Flask(__name__)\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up STT and TTS API clients 변환된 데이터값 변수로 받기\n",
    "stt_client = speech.SpeechClient()\n",
    "tts_client = texttospeech.TextToSpeechClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 사용자 음성 녹음\n",
    "def record_audio():\n",
    "    CHUNK = 1024\n",
    "    FORMAT = pyaudio.paInt16  # 16 bit\n",
    "    CHANNELS = 1  # mono\n",
    "    RATE = 16000  # sample rate\n",
    "    SILENCE_THRESHOLD = 300  # silence threshold\n",
    "    SILENT_CHUNKS = int(3.0 * RATE / CHUNK)  # 2.5 seconds of silence\n",
    "    MAX_RECORD_SECONDS = 5  # maximum recording time in seconds\n",
    "\n",
    "    # PyAudio 인스턴스 초기화\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    # 스트림 열기\n",
    "    stream = p.open(format=FORMAT,\n",
    "                    channels=CHANNELS,\n",
    "                    rate=RATE,\n",
    "                    input=True,\n",
    "                    frames_per_buffer=CHUNK)\n",
    "\n",
    "    print(\"* recording\")\n",
    "\n",
    "    frames = []\n",
    "    silent_chunks = 0\n",
    "    audio_started = False\n",
    "    start_time = None\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            data = stream.read(CHUNK, exception_on_overflow=False)\n",
    "            frames.append(data)\n",
    "\n",
    "            # Check for silence\n",
    "            audio_data = np.frombuffer(data, dtype=np.int16)\n",
    "            if np.abs(audio_data).mean() < SILENCE_THRESHOLD:\n",
    "                silent_chunks += 1\n",
    "                if silent_chunks > SILENT_CHUNKS:\n",
    "                    break\n",
    "            else:\n",
    "                silent_chunks = 0   \n",
    "\n",
    "            if audio_started:\n",
    "                if silent_chunks > SILENT_CHUNKS or (time.time() - start_time) >= MAX_RECORD_SECONDS:\n",
    "                    break\n",
    "            elif silent_chunks == 0:\n",
    "                audio_started = True\n",
    "                start_time = time.time()\n",
    "\n",
    "    finally:\n",
    "        print(\"* done recording\")\n",
    "\n",
    "        # 스트림 종료\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        # PyAudio 인스턴스 종료\n",
    "        p.terminate()\n",
    "\n",
    "    # Save the recorded audio to a WAV file\n",
    "    wf = wave.open(\"audio.wav\", \"wb\")\n",
    "    wf.setnchannels(CHANNELS)\n",
    "    wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "    wf.setframerate(RATE)\n",
    "    wf.writeframes(b\"\".join(frames))\n",
    "    wf.close()\n",
    "\n",
    "    # Return the recorded audio data\n",
    "    return b\"\".join(frames)\n",
    "\n",
    "## Byte 오디오 데이터를 한글 text로 변환 STT\n",
    "def transcribe_audio(audio_data):\n",
    "    audio = speech.RecognitionAudio(content=audio_data)\n",
    "    config = speech.RecognitionConfig(\n",
    "        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "        sample_rate_hertz=16000,\n",
    "        language_code=\"ko-KR\"\n",
    "    )\n",
    "\n",
    "    response = stt_client.recognize(config=config, audio=audio)\n",
    "    \n",
    "    if response.results:\n",
    "        return response.results[0].alternatives[0].transcript\n",
    "    else:\n",
    "        print(\"음성 인식에 실패했습니다. 다시 시도해주세요.\")\n",
    "        return None\n",
    "    \n",
    "def synthesize_speech(text, voice_index=0):\n",
    "    # Ensure the provided index is valid\n",
    "    if voice_index < 0 or voice_index >= len(speech_params_list):\n",
    "        raise ValueError(\"Invalid voice index provided\")\n",
    "\n",
    "    # Choose the voice settings based on the provided index\n",
    "    speech_params = speech_params_list[voice_index]\n",
    "\n",
    "    # Convert the ssml_gender string to the corresponding enum value\n",
    "    ssml_gender_map = {\n",
    "        \"MALE\": texttospeech.SsmlVoiceGender.MALE,\n",
    "        \"FEMALE\": texttospeech.SsmlVoiceGender.FEMALE,\n",
    "    }\n",
    "    ssml_gender = ssml_gender_map[speech_params[\"ssml_gender\"]]\n",
    "\n",
    "    input_text = texttospeech.SynthesisInput(text=text)\n",
    "    voice = texttospeech.VoiceSelectionParams(\n",
    "        language_code=\"ko-KR\",\n",
    "        ssml_gender=ssml_gender,  # Use the enum value instead of string\n",
    "        name=speech_params[\"voice_name\"]\n",
    "    )\n",
    "    \n",
    "    audio_config = texttospeech.AudioConfig(\n",
    "        audio_encoding=texttospeech.AudioEncoding.LINEAR16,\n",
    "        speaking_rate=speech_params[\"speaking_rate\"],\n",
    "        pitch=speech_params[\"pitch\"]\n",
    "    )\n",
    "\n",
    "    response = tts_client.synthesize_speech(\n",
    "        input=input_text, voice=voice, audio_config=audio_config\n",
    "    )\n",
    "\n",
    "    return response.audio_content\n",
    "\n",
    "## pyaudio를 사용하여 음성을 재생하는 함수\n",
    "def play_audio(audio_data):\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(\n",
    "        format=p.get_format_from_width(2),\n",
    "        channels=1,\n",
    "        rate=24000,\n",
    "        output=True,\n",
    "    )\n",
    "    stream.write(audio_data)\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.mixer.init()\n",
    "\n",
    "def play_conversaion(bgm1, volume=0.5):\n",
    "    pygame.mixer.init()\n",
    "    pygame.mixer.music.load(bgm1)\n",
    "    pygame.mixer.music.set_volume(volume)  # 볼륨 설정\n",
    "    pygame.mixer.music.play(0)  # -1은 무한 반복, 0은 한번 재생을 의미\n",
    "\n",
    "def stop_music():\n",
    "    pygame.mixer.music.stop()\n",
    "    \n",
    "bgm1 = \"/Users/kihunlee/Desktop/2312 RA 전시 개발/1_humanoid/hu_bgm1.mp3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프롬프트설계, 과거기억 데이터 확인\n",
    "messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesize(text):\n",
    "    url = \"https://api.elevenlabs.io/v1/text-to-speech/LcfcDJNUP1GQjkzn1xUU\"\n",
    "    headers = {\n",
    "        \"Accept\": \"audio/mpeg\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"xi-api-key\": \"7d0080db305aac73a5d239d1b5c124b2\"  # 여기에 실제 API 키를 입력해야 합니다. 내 키\n",
    "    }\n",
    "    data = {\n",
    "        \"text\": text,\n",
    "        \"model_id\": \"eleven_multilingual_v2\",\n",
    "        \"voice_settings\": {\n",
    "            \"stability\": 0.5,\n",
    "            \"similarity_boost\": 0.75,\n",
    "            \"style_exaggeration\": 0.0\n",
    "        }\n",
    "    }\n",
    "    response = requests.post(url, json=data, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        audio_stream = io.BytesIO(response.content)\n",
    "        audio = AudioSegment.from_file(audio_stream, format=\"mp3\")\n",
    "        play(audio)\n",
    "    else:\n",
    "        print(\"Error: \", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#대화로직\n",
    "def conv1():  \n",
    "    print(\"사람응답 1-1 \")\n",
    "    user_audio = record_audio()\n",
    "    user_content = transcribe_audio(user_audio)\n",
    "    print(user_content)\n",
    "\n",
    "    if not user_content:\n",
    "        user_content = \"모르겠어.\"\n",
    "    elif \"그만\" in user_content:\n",
    "        conv_end = \"그래. 아쉽네. 다음에 또 대화하자구 친구.\"\n",
    "        synthesize(conv_end)\n",
    "        return 1  # 사용자가 '그만'이라고 말하면 함수 종료\n",
    "\n",
    "    messages.append({\"role\": \"system\", \"content\": \"다음 아래 사용자 발화에 대해 총 두 문장으로 대답해줘. 마지막 문장은 질문으로 대답해줘\"})\n",
    "    messages.append({\"role\": \"user\", \"content\": user_content}) #사용자 발화 추가\n",
    "\n",
    "    # 대답1 Use GPT API to generate response\n",
    "    completion = openai.ChatCompletion.create(model=\"gpt-4\", messages=messages)\n",
    "    assistant_content = completion.choices[0].message[\"content\"].strip()\n",
    "    messages.append({\"role\": \"assistant\", \"content\": f\"{assistant_content}\"})\n",
    "    print(f\"S.I : {assistant_content}\")\n",
    "\n",
    "    print(\"어른 대화 2 \")\n",
    "    synthesize(assistant_content) #elevenlabs API TTS\n",
    "    return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_conversaion(bgm1, 0.5) #카페분위기\n",
    "\n",
    "# 처음에 역할을 정의하고 시작합니다.\n",
    "assistant_role = \"'커피앤 시가렛'이라는 연극의 배우, 이기팝\"\n",
    "assistant_persona = \"건방지고 반말하는\"\n",
    "user_role = \"친구\"\n",
    "messages.append({\"role\": \"system\", \"content\": f\"GPT(:{assistant_role})와 사용자(:{user_role})와의 역할극을 시작합니다. {assistant_role}은 {assistant_persona} 성격으로 'coffee and cigarette' 작품에 대해 스몰토크를 합니다.\"})\n",
    "assistant_content = None\n",
    "\n",
    "print(\"대화시작-1\")\n",
    "conv_1 = \"오랜만이네, 친구..., 나 이따가 또 연극해야해.\"\n",
    "synthesize(conv_1) #elevenlabs API TTS\n",
    "time.sleep(1.0)\n",
    "messages = [{\"role\": \"system\", \"content\": conv_1}] #conv_GPT 추가\n",
    "\n",
    "while True:\n",
    "    # '그만'하면 종료\n",
    "    conv1()\n",
    "    if conv1() is 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. 연극에 대해 물어보고 \"응 너가 연극배우인거는 내가알고 있지, 오늘 무슨 커피엔 시가렛인가 그거 연기한다고 했었나?\"\n",
    "#2. 인간의 감정 표현 물어보고\n",
    "#3. 인간과 기계의 공존에 대해 물어보고\n",
    "#4. 관람객에게 인사하면 종료 \"우리의 대화를 처다보고 있는 사람들이 있어. 알고있어?\" \"이기 근데 오른쪽 봐봐, 누가 우리를 계속 처다보고 있어\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_music()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
